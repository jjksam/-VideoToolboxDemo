//
//  SuperVideoFrameExtractor.m
//  testFrameExtractor
//
//  Created by htaiwan on 10/24/14.
//  Copyright (c) 2014 appteam. All rights reserved.
//

#import "SuperVideoFrameExtractor.h"
#import "AudioStreamer.h"
#import "Utilities.h"

#include "libavutil/intreadwrite.h"
#include "avcodec.h"
#include "videotoolbox.h"

#define RTSP_DUMP_DATA 1
#define USE_FFMPEG_DECODE 0

@interface SuperVideoFrameExtractor ()
{
    AVVideotoolboxContext *avVideotoolBox;
    CMVideoFormatDescriptionRef videoFormatDescr;
    VTDecompressionSessionRef session;
    OSStatus status;
    int is_avc;
    NSData *spsData;
    NSData *ppsData;
}
@property (nonatomic, retain) AudioStreamer *audioController;
-(void)convertFrameToRGB;
-(UIImage *)imageFromAVPicture:(AVPicture)pict width:(int)width height:(int)height;
-(void)savePicture:(AVPicture)pict width:(int)width height:(int)height index:(int)iFrame;
-(void)setupScaler;

@end

@implementation SuperVideoFrameExtractor
@synthesize outputWidth, outputHeight;

@synthesize audioPacketQueue,audioPacketQueueSize;
@synthesize _audioStream,_audioCodecContext;
@synthesize emptyAudioBuffer;

#pragma mark - property method

-(void)setOutputWidth:(int)newValue
{
    if (outputWidth == newValue) return;
    outputWidth = newValue;
    [self setupScaler];
}

-(void)setOutputHeight:(int)newValue
{
    if (outputHeight == newValue) return;
    outputHeight = newValue;
    [self setupScaler];
}

-(int)sourceWidth
{
    return pCodecCtx->width;
}

-(int)sourceHeight
{
    return pCodecCtx->height;
}

-(UIImage *)currentImage
{
    if (!pFrame->data[0]) return nil;
    [self convertFrameToRGB];
    
    [self savePicture:picture width:outputWidth height:outputWidth index:frameIndex];
    frameIndex ++;
    
    return [self imageFromAVPicture:picture width:outputWidth height:outputHeight];
}

-(double)duration
{
    return (double)pFormatCtx->duration / AV_TIME_BASE;
}

-(double)currentTime
{
    AVRational timeBase = pFormatCtx->streams[videoStream]->time_base;
    return packet.pts * (double)timeBase.num / timeBase.den;
}



#pragma mark - Object method

// For stream server test
- (id)initWithVideo:(NSString *)moviePath usesTcp:(BOOL)usesTcp
{
    if (!(self=[super init])) return nil;
    
    AVCodec         *pCodec;
    
    // 註冊所有format和codecs
    avcodec_register_all();
    av_register_all();
    avformat_network_init();
    pFormatCtx = avformat_alloc_context();
    // Set the RTSP Options
    AVDictionary *opts = 0;
    if (usesTcp) {
        av_dict_set(&opts, "rtsp_transport", "tcp", 0);
    }
    
    // 打開影片檔案
    if (avformat_open_input(&pFormatCtx, [moviePath cStringUsingEncoding:NSASCIIStringEncoding], NULL, &opts) != 0) {
        av_log(NULL, AV_LOG_ERROR, "Couldn't open file\n");
        goto initError;
    }
    
    pFormatCtx->max_analyze_duration = 951200;
    pFormatCtx->probesize = 951200;
    // 取得影片串流資訊
    if (avformat_find_stream_info(pFormatCtx,NULL) < 0) {
        av_log(NULL, AV_LOG_ERROR, "Couldn't find stream information\n");
        goto initError;
    }
    
    // Find the first video stream
    videoStream=-1;
    audioStream=-1;
    
    for (int i=0; i<pFormatCtx->nb_streams; i++) {
        if (pFormatCtx->streams[i]->codec->codec_type == AVMEDIA_TYPE_VIDEO) {
            NSLog(@"found video stream");
            videoStream=i;
        }
        if (pFormatCtx->streams[i]->codec->codec_type == AVMEDIA_TYPE_AUDIO) {
            NSLog(@"found audio stream");
            audioStream=i;
        }
    }
    
    if (videoStream==-1 && audioStream==-1) {
        goto initError;
    }
    
    // Get a pointer to the codec context for the video stream
    pCodecCtx = pFormatCtx->streams[videoStream]->codec;
    
    // Alex: Data init
    spsData = nil;
    ppsData = nil;
    videoFormatDescr = NULL;
    session = NULL;
    
    // Find the decoder for the video stream
    pCodec = avcodec_find_decoder(pCodecCtx->codec_id);
    if (pCodec == NULL) {
        av_log(NULL, AV_LOG_ERROR, "Unsupported codec!\n");
        goto initError;
    }
    
    // Open codec
    if (avcodec_open2(pCodecCtx, pCodec, NULL) < 0) {
        av_log(NULL, AV_LOG_ERROR, "Cannot open video decoder\n");
        goto initError;
    }
    
    if (audioStream > -1 ) {
        NSLog(@"set up audiodecoder");
        [self setupAudioDecoder];
    }
    
    // Allocate video frame
//    pFrame = avcodec_alloc_frame();
    pFrame = av_frame_alloc();
    
    outputWidth = pCodecCtx->width;
    self.outputHeight = pCodecCtx->height;
    if (opts != NULL)
        free(opts);
    return self;

initError:
    if (opts != NULL)
        free(opts);
    [self release];
    return nil;
}

// For local File test
-(id)initWithVideo:(NSString *)moviePath
{
    if (!(self=[super init])) return nil;
    
    AVCodec         *pCodec;

    frameIndex = 0;
    
    // 註冊所有format和codecs
    avcodec_register_all();
    av_register_all();
    
    // 打開影片檔案
    if (avformat_open_input(&pFormatCtx,[moviePath cStringUsingEncoding:NSASCIIStringEncoding], NULL, NULL) != 0) {
        av_log(NULL, AV_LOG_ERROR, "Couldn't open file\n");
        goto initError;
    }
    
    // 取得影片串流資訊
    if (avformat_find_stream_info(pFormatCtx, NULL) < 0) {
        av_log(NULL, AV_LOG_ERROR, "Couldn't find stream information\n");
        goto initError;
    }
    
    // 取得第一個影片串流
    if ((videoStream = av_find_best_stream(pFormatCtx, AVMEDIA_TYPE_VIDEO, -1, -1, &pCodec, 0)) < 0) {
        av_log(NULL, AV_LOG_ERROR, "Cannot find a video stream in the input file\n");
        goto initError;
    }
    
    // 取得影片串流的codec context的指標
    pCodecCtx = pFormatCtx->streams[videoStream]->codec;
    
    // 找出此影片串流的decoder
    pCodec = avcodec_find_decoder(pCodecCtx->codec_id);
    
    // 開啟 codec
    if(avcodec_open2(pCodecCtx, pCodec, NULL) < 0) {
        av_log(NULL, AV_LOG_ERROR, "Cannot open video decoder\n");
        goto initError;
    }
    
    // Allocate video frame
    pFrame = av_frame_alloc();
    
    outputWidth = pCodecCtx->width;
    self.outputHeight = pCodecCtx->height;
    
    return self;

initError:
    [self release];
    return nil;    
}


- (CMSampleBufferRef)cmSampleBufferFromCGImage:(CGImageRef)image size:(CGSize)size
{
    NSDictionary *options = [NSDictionary dictionaryWithObjectsAndKeys:
                             [NSNumber numberWithBool:YES], kCVPixelBufferCGImageCompatibilityKey,
                             [NSNumber numberWithBool:YES], kCVPixelBufferCGBitmapContextCompatibilityKey,
                             nil];
    CVPixelBufferRef pxbuffer = NULL;
    
    CVReturn status = CVPixelBufferCreate(kCFAllocatorDefault, size.width,
                                          size.height, kCVPixelFormatType_32ARGB, (CFDictionaryRef) options,
                                          &pxbuffer);
    NSParameterAssert(status == kCVReturnSuccess && pxbuffer != NULL);
    
    CVPixelBufferLockBaseAddress(pxbuffer, 0);
    void *pxdata = CVPixelBufferGetBaseAddress(pxbuffer);
    NSParameterAssert(pxdata != NULL);
    
    CGColorSpaceRef rgbColorSpace = CGColorSpaceCreateDeviceRGB();
    CGContextRef context = CGBitmapContextCreate(pxdata, size.width,
                                                 size.height, 8, 4*size.width, rgbColorSpace,
                                                 kCGImageAlphaNoneSkipFirst);
    NSParameterAssert(context);
    CGContextConcatCTM(context, CGAffineTransformMakeRotation(0));
    CGContextDrawImage(context, CGRectMake(0, 0, CGImageGetWidth(image),
                                           CGImageGetHeight(image)), image);
    CGColorSpaceRelease(rgbColorSpace);
    CGContextRelease(context);
    CVPixelBufferUnlockBaseAddress(pxbuffer, 0);
    CMVideoFormatDescriptionRef videoInfo = NULL;
    CMSampleBufferRef sampleBuffer = NULL;
    CMSampleBufferCreateForImageBuffer(kCFAllocatorDefault,
                                       pxbuffer, true, NULL, NULL, videoInfo, NULL, &sampleBuffer);
    return sampleBuffer;
}

-(CGImageRef)CGImageRefFromAVPicture:(AVPicture)pict width:(int)width height:(int)height
{
    CGBitmapInfo bitmapInfo = kCGBitmapByteOrderDefault;
    CFDataRef data = CFDataCreateWithBytesNoCopy(kCFAllocatorDefault, pict.data[0], pict.linesize[0]*height,kCFAllocatorNull);
    CGDataProviderRef provider = CGDataProviderCreateWithCFData(data);
    CGColorSpaceRef colorSpace = CGColorSpaceCreateDeviceRGB();
    CGImageRef cgImage = CGImageCreate(width,
                                       height,
                                       8,
                                       24,
                                       pict.linesize[0],
                                       colorSpace,
                                       bitmapInfo, 
                                       provider, 
                                       NULL, 
                                       NO, 
                                       kCGRenderingIntentDefault);
    CGColorSpaceRelease(colorSpace);
    UIImage *image = [UIImage imageWithCGImage:cgImage];
    CGImageRelease(cgImage);
    CGDataProviderRelease(provider);
    CFRelease(data);

    return image.CGImage;
}

// 根據指定時間去找尋最近的keyframe
-(void)seekTime:(double)seconds
{
    AVRational timeBase = pFormatCtx->streams[videoStream]->time_base;
    int64_t targetFrame = (int64_t)((double)timeBase.den / timeBase.num * seconds);
    avformat_seek_file(pFormatCtx, videoStream, targetFrame, targetFrame, targetFrame, AVSEEK_FLAG_FRAME);
    avcodec_flush_buffers(pCodecCtx);
}

#pragma mark - private method 

- (void)setupAudioDecoder
{
    if (audioStream >= 0) {
        _audioBufferSize = 192000;
        _audioBuffer = av_malloc(_audioBufferSize);
        _inBuffer = NO;
        
        _audioCodecContext = pFormatCtx->streams[audioStream]->codec;
        _audioStream = pFormatCtx->streams[audioStream];
        
        AVCodec *codec = avcodec_find_decoder(_audioCodecContext->codec_id);
        if (codec == NULL) {
            NSLog(@"Not found audio codec.");
            return;
        }
        
        if (avcodec_open2(_audioCodecContext, codec, NULL) < 0) {
            NSLog(@"Could not open audio codec.");
            return;
        }
        
        if (audioPacketQueue) {
            [audioPacketQueue release];
            audioPacketQueue = nil;
        }
        audioPacketQueue = [[NSMutableArray alloc] init];
        
        if (audioPacketQueueLock) {
            [audioPacketQueueLock release];
            audioPacketQueueLock = nil;
        }
        audioPacketQueueLock = [[NSLock alloc] init];

        if (_audioController) {
            [_audioController _stopAudio];
            [_audioController release];
            _audioController = nil;
        }
        _audioController = [[AudioStreamer alloc] initWithStreamer:self];
    } else {
        pFormatCtx->streams[audioStream]->discard = AVDISCARD_ALL;
        audioStream = -1;
    }
}

- (void)nextPacket
{
    _inBuffer = NO;
}

- (AVPacket*)readPacket
{
    if (_currentPacket.size > 0 || _inBuffer) return &_currentPacket;
    
    NSMutableData *packetData = [audioPacketQueue objectAtIndex:0];
    _packet = [packetData mutableBytes];
    
    if (_packet) {
        if (_packet->dts != AV_NOPTS_VALUE) {
            _packet->dts += av_rescale_q(0, AV_TIME_BASE_Q, _audioStream->time_base);
        }
        
        if (_packet->pts != AV_NOPTS_VALUE) {
            _packet->pts += av_rescale_q(0, AV_TIME_BASE_Q, _audioStream->time_base);
        }
        
        [audioPacketQueueLock lock];
        audioPacketQueueSize -= _packet->size;
        if ([audioPacketQueue count] > 0) {
            [audioPacketQueue removeObjectAtIndex:0];
        }
        [audioPacketQueueLock unlock];
        
        _currentPacket = *(_packet);
    }
    
    return &_currentPacket;
}

- (void)startAudio {
    [_audioController _startAudio];
}

- (void)closeAudio
{
    [_audioController _stopAudio];
    primed=NO;
}


-(void)setupScaler
{
    // release old pricture 和 scaler
    avpicture_free(&picture);
    sws_freeContext(img_convert_ctx);
    
    // Allocate RGB picture
    avpicture_alloc(&picture, AV_PIX_FMT_RGB24, outputWidth, outputHeight);
    
    // 建立scaler
    static int sws_flags =  SWS_FAST_BILINEAR;
    img_convert_ctx = sws_getContext(pCodecCtx->width,
                                     pCodecCtx->height,
                                     pCodecCtx->pix_fmt,
                                     outputWidth,
                                     outputHeight,
                                     AV_PIX_FMT_RGB24,
                                     sws_flags, NULL, NULL, NULL);
}

-(void)convertFrameToRGB
{
    sws_scale(img_convert_ctx, pFrame->data, pFrame->linesize, 0, pCodecCtx->height, picture.data, picture.linesize);
}

-(UIImage *)imageFromAVPicture:(AVPicture)pict width:(int)width height:(int)height
{
    CGBitmapInfo bitmapInfo = kCGBitmapByteOrderDefault;
    CFDataRef data = CFDataCreateWithBytesNoCopy(kCFAllocatorDefault, pict.data[0], pict.linesize[0]*height, kCFAllocatorNull);
    CGDataProviderRef provider = CGDataProviderCreateWithCFData(data);
    CGColorSpaceRef colorSpace = CGColorSpaceCreateDeviceRGB();
    CGImageRef cgImage = CGImageCreate(width,
                                       height,
                                       8,
                                       24,
                                       pict.linesize[0],
                                       colorSpace,
                                       bitmapInfo,
                                       provider,
                                       NULL,
                                       NO,
                                       kCGRenderingIntentDefault);
    CGColorSpaceRelease(colorSpace);
    UIImage *image = [UIImage imageWithCGImage:cgImage];
    CGImageRelease(cgImage);
    CGDataProviderRelease(provider);
    CFRelease(data);
    
    return image;
}

-(void)savePicture:(AVPicture)pict width:(int)width height:(int)height index:(int)iFrame
{
    FILE *pFile;
    NSString *fileName;
    int  y;
    
    fileName = [Utilities documentsPath:[NSString stringWithFormat:@"image%04d.ppm",iFrame]];
    // Open file
    NSLog(@"write image file: %@",fileName);
    pFile=fopen([fileName cStringUsingEncoding:NSASCIIStringEncoding], "wb");
    if(pFile==NULL)
        return;
    
    // Write header
    fprintf(pFile, "P6\n%d %d\n255\n", width, height);
    
    // Write pixel data
    for(y=0; y<height; y++)
        fwrite(pict.data[0]+y*pict.linesize[0], 1, width*3, pFile);
    
    // Close file
    fclose(pFile);
}

-(void)dealloc {
    /* Clean up. */
    if (session != NULL) {
        VTDecompressionSessionInvalidate(session);
        CFRelease(session);
    }
    session = NULL;
    if (videoFormatDescr != NULL) {
        CFRelease(videoFormatDescr);
    }
    videoFormatDescr = NULL;
    
    // Free scaler
    sws_freeContext(img_convert_ctx);
    
    // Free RGB picture
    avpicture_free(&picture);
    
    // Free the packet that was allocated by av_read_frame
    av_free_packet(&packet);
    
    // Free the YUV frame
    av_free(pFrame);
    
    // Close the codec
    if (pCodecCtx) avcodec_close(pCodecCtx);
    
    // Close the video file
    if (pFormatCtx) avformat_close_input(&pFormatCtx);
    
    [super dealloc];
}


#pragma mark - iOS8 HW decode 相關method

- (void)iOS8HWDecode
{
    // 1. get SPS,PPS form stream data, and create CMFormatDescription 和 VTDecompressionSession
    if (spsData == nil && ppsData == nil) {
        uint8_t *data = pCodecCtx->extradata;
        int size = pCodecCtx->extradata_size;
#if DEBUG
        NSData *tmp1 = [NSData dataWithBytes:data length:size];
        [self dumpData:tmp1];
#endif
        
        if (!data || size <= 0)
            return;
        if (data[0] == 1) {
            int i, cnt, nalsize;
            const uint8_t *p = data;
            is_avc = 1;
            if (size < 7) {
               // error
                NSLog(@"avcC %d too short", size);
            }
            
            // Decode sps from avcC
            cnt = *(p + 5) & 0x1f; // Number of sps
            p  += 6;
            for (i = 0; i < cnt; i++) {
                nalsize = AV_RB16(p) + 2;
                if (nalsize > size - (p - data))
                    return;
                spsData = [NSData dataWithBytes:p + 2 length:nalsize - 2];
//                ret = decode_extradata_ps_mp4(p, nalsize, ps, err_recognition, logctx);
                if (spsData <= 0) {
                    NSLog(@"Decoding sps %d from avcC failed", i);
                    return;
                }
                p += nalsize;
            }
            // Decode pps from avcC
            cnt = *(p++); // Number of pps
            for (i = 0; i < cnt; i++) {
                nalsize = AV_RB16(p) + 2;
                if (nalsize > size - (p - data))
                    return;
                ppsData = [NSData dataWithBytes:p + 2 length:nalsize - 2];
//                ret = decode_extradata_ps_mp4(p, nalsize, ps, err_recognition, logctx);
                if (ppsData.length <= 0) {
                    NSLog(@"Decoding pps %d from avcC failed", i);
                    return;
                }
                p += nalsize;
            }
//            return; // skip
            // Store right nal length size that will be used to parse all other nals
//            *nal_length_size = (data[4] & 0x03) + 1;
        } else {
            // not avcC, try to find the start code 00 00 00 01 just check the libavcodec/h264_parse.c int ff_h264_decode_extradata
            is_avc = 0;
        
            int startCodeSPSIndex = 0;
            int startCodePPSIndex = 0;
            int spsLength = 0;
            int ppsLength = 0;

            for (int i = 0; i < size; i++) {
                if (i >= 3) {
                    if (data[i] == 0x01 && data[i-1] == 0x00 && data[i-2] == 0x00 && data[i-3] == 0x00) {
                        if (startCodeSPSIndex == 0) {
                            startCodeSPSIndex = i;
                        }
                        if (i > startCodeSPSIndex) {
                            startCodePPSIndex = i;
                        }
                    }
                }
            }
            if (startCodePPSIndex <= 3 && startCodeSPSIndex <= 3) {
                return;
            }
            spsLength = startCodePPSIndex - startCodeSPSIndex - 4;
            ppsLength = size - (startCodePPSIndex + 1);
                
            
            NSLog(@"startCodeSPSIndex --> %i",startCodeSPSIndex);
            NSLog(@"startCodePPSIndex --> %i",startCodePPSIndex);
            NSLog(@"spsLength --> %i",spsLength);
            NSLog(@"ppsLength --> %i",ppsLength);

            int nalu_type;
            nalu_type = ((uint8_t) data[startCodeSPSIndex + 1] & 0x1F);
            NSLog(@"NALU with Type \"%@\" received.", naluTypesStrings[nalu_type]);
            if (nalu_type == 7) {
//                [self dumpData:[NSData dataWithBytesNoCopy:packet.data length:packet.size freeWhenDone:NO]];
                spsData = [NSData dataWithBytes:&(data[startCodeSPSIndex + 1]) length:spsLength];
            }
            
            nalu_type = ((uint8_t) data[startCodePPSIndex + 1] & 0x1F);
            NSLog(@"NALU with Type \"%@\" received.", naluTypesStrings[nalu_type]);
            if (nalu_type == 8) {
//                [self dumpData:[NSData dataWithBytesNoCopy:packet.data length:packet.size freeWhenDone:NO]];
                ppsData = [NSData dataWithBytes:&(data[startCodePPSIndex + 1]) length:ppsLength];
            }
        }

        // 2. create  CMFormatDescription
        if (spsData != nil && ppsData != nil) {
            const uint8_t* const parameterSetPointers[2] = { (const uint8_t*)[spsData bytes], (const uint8_t*)[ppsData bytes] };
            const size_t parameterSetSizes[2] = { [spsData length], [ppsData length] };
            status = CMVideoFormatDescriptionCreateFromH264ParameterSets(kCFAllocatorDefault, 2, parameterSetPointers, parameterSetSizes, 4, &videoFormatDescr);
            NSLog(@"Found all data for CMVideoFormatDescription. Creation: %@.", (status == noErr) ? @"successfully." : @"failed.");
        }

        // 3. create VTDecompressionSession
        VTDecompressionOutputCallbackRecord callback;
        callback.decompressionOutputCallback = didDecompress;
        callback.decompressionOutputRefCon = (__bridge void *)self;
        NSDictionary *destinationImageBufferAttributes = @{(id)kCVPixelBufferOpenGLESCompatibilityKey: @(NO), (id)kCVPixelBufferPixelFormatTypeKey:@(kCVPixelFormatType_32BGRA)};
//          NSDictionary *destinationImageBufferAttributes =[NSDictionary dictionaryWithObjectsAndKeys:[NSNumber numberWithBool:YES],(id)kCVPixelBufferOpenGLESCompatibilityKey,[NSNumber numberWithInt:kCVPixelFormatType_420YpCbCr8BiPlanarFullRange],(id)kCVPixelBufferPixelFormatTypeKey,nil];
//        NSDictionary *destinationImageBufferAttributes =[NSDictionary dictionaryWithObjectsAndKeys:[NSNumber numberWithBool:NO],(id)kCVPixelBufferOpenGLESCompatibilityKey,nil];
//        NSDictionary *destinationImageBufferAttributes = [NSDictionary dictionaryWithObject: [NSNumber numberWithInt:kCVPixelFormatType_32BGRA] forKey: (id)kCVPixelBufferPixelFormatTypeKey];
        status = VTDecompressionSessionCreate(kCFAllocatorDefault, videoFormatDescr, NULL, (CFDictionaryRef)destinationImageBufferAttributes, &callback, &session);
        NSLog(@"Creating Video Decompression Session: %@.", (status == noErr) ? @"successfully." : @"failed.");
        
//        AVVideotoolboxContext *avVideotoolBox = av_videotoolbox_alloc_context();
        int ret = -1;
//        int ret = av_videotoolbox_default_init(pCodecCtx);
//        NSLog(@"Creating Video Decompression Session: %@.", (ret >= 0) ? @"successfully." : @"failed.");
        if (ret >= 0) {
            avVideotoolBox = pCodecCtx->hwaccel_context;
            videoFormatDescr = avVideotoolBox->cm_fmt_desc;
            session = avVideotoolBox->session;
        }

        int32_t timeSpan = 90000;
        CMSampleTimingInfo timingInfo;
        timingInfo.presentationTimeStamp = CMTimeMake(0, timeSpan);
        timingInfo.duration =  CMTimeMake(3000, timeSpan);
        timingInfo.decodeTimeStamp = kCMTimeInvalid;
        return; // skip
    }
    
    uint8_t *data = pCodecCtx->extradata;
    int size = pCodecCtx->extradata_size;
#if DEBUG
    NSLog(@"dump extradata");
    NSData *tmp1 = [NSData dataWithBytes:data length:size];
    [self dumpData:tmp1];
#endif
//    [self dumpData:[NSData dataWithBytesNoCopy:packet.data length:packet.size freeWhenDone:NO]];
    int nalu_type = 0;
    if (is_avc) {
        nalu_type = 1;
    } else {
        int startCodeIndex = 0;
        for (int i = 0; i < 5; i++) {
            if (packet.data[i] == 0x01) {
                startCodeIndex = i;
                break;
            }
        }
        nalu_type = ((uint8_t)packet.data[startCodeIndex + 1] & 0x1F);
        NSLog(@"NALU with Type \"%@\" received.", naluTypesStrings[nalu_type]);
    }
    if (nalu_type == 1 || nalu_type == 5) {
        if (nalu_type == 5) {
            NSLog(@"nalu_type : 5, IDR frame");
        }
        // 4. get NALUnit payload into a CMBlockBuffer,
        CMBlockBufferRef videoBlock = NULL;
        status = CMBlockBufferCreateWithMemoryBlock(NULL, packet.data, packet.size, kCFAllocatorNull, NULL, 0, packet.size, 0, &videoBlock);
        NSLog(@"BlockBufferCreation: %@", (status == kCMBlockBufferNoErr) ? @"successfully." : @"failed.");
       
        // 5.  making sure to replace the separator code with a 4 byte length code (the length of the NalUnit including the unit code)
        int removeHeaderSize = packet.size - 4;
        const uint8_t sourceBytes[] = {(uint8_t)(removeHeaderSize >> 24), (uint8_t)(removeHeaderSize >> 16), (uint8_t)(removeHeaderSize >> 8), (uint8_t)removeHeaderSize};
        status = CMBlockBufferReplaceDataBytes(sourceBytes, videoBlock, 0, 4);
        NSLog(@"BlockBufferReplace: %@", (status == kCMBlockBufferNoErr) ? @"successfully." : @"failed.");
        
        NSString *tmp3 = [NSString new];
        for(int i = 0; i < sizeof(sourceBytes); i++) {
            NSString *str = [NSString stringWithFormat:@" %.2X",sourceBytes[i]];
            tmp3 = [tmp3 stringByAppendingString:str];
        }
        NSLog(@"size = %i , 16Byte = %@",removeHeaderSize,tmp3);

        // 6. create a CMSampleBuffer.
        CMSampleBufferRef sbRef = NULL;
//        int32_t timeSpan = 90000;
//        CMSampleTimingInfo timingInfo;
//        timingInfo.presentationTimeStamp = CMTimeMake(0, timeSpan);
//        timingInfo.duration =  CMTimeMake(3000, timeSpan);
//        timingInfo.decodeTimeStamp = kCMTimeInvalid;
        const size_t sampleSizeArray[] = {packet.size};
//        status = CMSampleBufferCreate(kCFAllocatorDefault, videoBlock, true, NULL, NULL, videoFormatDescr, 1, 1, &timingInfo, 1, sampleSizeArray, &sbRef);
        status = CMSampleBufferCreate(kCFAllocatorDefault, videoBlock, true, NULL, NULL, videoFormatDescr, 1, 0, NULL, 1, sampleSizeArray, &sbRef);

        NSLog(@"SampleBufferCreate: %@", (status == noErr) ? @"successfully." : @"failed.");
        
        // 7. use VTDecompressionSessionDecodeFrame
        VTDecodeFrameFlags flags = kVTDecodeFrame_EnableAsynchronousDecompression;
        VTDecodeInfoFlags flagOut;
        status = VTDecompressionSessionDecodeFrame(session, sbRef, flags, &sbRef, &flagOut);
        NSLog(@"VTDecompressionSessionDecodeFrame: %@", (status == noErr) ? @"successfully." : @"failed.");
        
        CFRelease(sbRef);
        
        [self.delegate startDecodeData];

//        /* Flush in-process frames. */
        VTDecompressionSessionFinishDelayedFrames(session);
//        /* Block until our callback has been called with the last frame. */
        VTDecompressionSessionWaitForAsynchronousFrames(session);
//
//        /* Clean up. */
//        VTDecompressionSessionInvalidate(session);
//        CFRelease(session);
//        CFRelease(videoFormatDescr);


//        NSLog(@"========================================================================");
//        NSLog(@"========================================================================");
    }
}

#pragma mark - Debug Helper
- (void)dumpData:(NSData *)data
{
#if RTSP_DUMP_DATA
    const UInt8* tmpData = data.bytes;
    NSUInteger length = [data length];
    int c = 0;
    for (int i = 0; i< length; i++) {
        printf("%02x ", tmpData[i]);
        c++;
        if (c >= 20)
        {
            c = 0;
            printf("\n");
        }
    }
    
    printf("\n");
#endif
}


// 判斷在video stream中是否還有下一個fram可以讀取，回傳false，表示影片已經播放完畢
-(BOOL)stepFrame
{
    // AVPacket packet;
    int ret = 0; // success
    int frameFinished = 0;
    while (!ret && av_read_frame(pFormatCtx, &packet)>= 0) {
        // 確認packet 是否是屬於此video stream
        if (packet.stream_index == videoStream) {
            
#warning  important: choose new iOS8 API start to decode

#if USE_FFMPEG_DECODE
            // FFMPEG decode
#if USE_NEW_API
            ret = avcodec_send_packet(pCodecCtx, &packet);
            if (ret < 0 && ret != AVERROR(EAGAIN) && ret != AVERROR_EOF)
                continue;
            ret = avcodec_receive_frame(pCodecCtx, pFrame);
            if (ret < 0 && ret != AVERROR_EOF)
                continue;
            // if ret == 0 ?
#endif
            avcodec_decode_video2(pCodecCtx, pFrame, &frameFinished, &packet);
            CGImageRef cgImageRef = [self imageFromAVPicture:picture width:outputWidth height:outputHeight].CGImage;
            if (cgImageRef != NULL) {
                CVImageBufferRef pixelBuffer = [self pixelBufferFromCGImage:cgImageRef];
    //            __weak __block SuperVideoFrameExtractor *weakSelf = (__bridge SuperVideoFrameExtractor *)decompressionOutputRefCon;
                [self.delegate getDecodeImageData:pixelBuffer];
            }
#else
            // ios8 HW decode
            [self iOS8HWDecode];
#endif
        }
    }
    
    return frameFinished != 0;
}



#pragma mark - VideoToolBox Decompress Frame CallBack
/*
 This callback gets called everytime the decompresssion session decodes a frame
 */
void didDecompress( void *decompressionOutputRefCon, void *sourceFrameRefCon, OSStatus status, VTDecodeInfoFlags infoFlags, CVImageBufferRef imageBuffer, CMTime presentationTimeStamp, CMTime presentationDuration )
{
    if (status != noErr || !imageBuffer) {
        // error -8969 codecBadDataErr
        // -12909 The operation couldn’t be completed. (OSStatus error -12909.)
        NSLog(@"Error decompresssing frame at time: %.3f error: %d infoFlags: %u", (float)presentationTimeStamp.value/presentationTimeStamp.timescale, (int)status, (unsigned int)infoFlags);
        return;
    }
    
//    NSLog(@"Got frame data.\n");
//    NSLog(@"Success decompresssing frame at time: %.3f error: %d infoFlags: %u", (float)presentationTimeStamp.value/presentationTimeStamp.timescale, (int)status, (unsigned int)infoFlags);
    __block SuperVideoFrameExtractor *weakSelf = (__bridge SuperVideoFrameExtractor *)decompressionOutputRefCon;
    [weakSelf.delegate getDecodeImageData:imageBuffer];
}


- (void) dumpPacketData
{
    // Log dump
    int index = 0;
    NSString *tmp = [NSString string];
    for(int i = 0; i < packet.size; i++) {
        NSString *str = [NSString stringWithFormat:@" %.2X",packet.data[i]];
        if (i == 4) {
            NSString *header = [NSString stringWithFormat:@"%.2X",packet.data[i]];
            NSLog(@" header ====>> %@",header);
            if ([header isEqualToString:@"41"]) {
                NSLog(@"P Frame");
            }
            if ([header isEqualToString:@"65"]) {
                NSLog(@"I Frame");
            }
        }
        tmp = [tmp stringByAppendingString:str];
        index++;
        if (index == 16) {
            NSLog(@"%@",tmp);
            tmp = @"";
            index = 0;
        }
    }
}

NSString * const naluTypesStrings[] = {
    @"Unspecified (non-VCL)",
    @"Coded slice of a non-IDR picture (VCL)",
    @"Coded slice data partition A (VCL)",
    @"Coded slice data partition B (VCL)",
    @"Coded slice data partition C (VCL)",
    @"Coded slice of an IDR picture (VCL)",
    @"Supplemental enhancement information (SEI) (non-VCL)",
    @"Sequence parameter set (non-VCL)",
    @"Picture parameter set (non-VCL)",
    @"Access unit delimiter (non-VCL)",
    @"End of sequence (non-VCL)",
    @"End of stream (non-VCL)",
    @"Filler data (non-VCL)",
    @"Sequence parameter set extension (non-VCL)",
    @"Prefix NAL unit (non-VCL)",
    @"Subset sequence parameter set (non-VCL)",
    @"Reserved (non-VCL)",
    @"Reserved (non-VCL)",
    @"Reserved (non-VCL)",
    @"Coded slice of an auxiliary coded picture without partitioning (non-VCL)",
    @"Coded slice extension (non-VCL)",
    @"Coded slice extension for depth view components (non-VCL)",
    @"Reserved (non-VCL)",
    @"Reserved (non-VCL)",
    @"Unspecified (non-VCL)",
    @"Unspecified (non-VCL)",
    @"Unspecified (non-VCL)",
    @"Unspecified (non-VCL)",
    @"Unspecified (non-VCL)",
    @"Unspecified (non-VCL)",
    @"Unspecified (non-VCL)",
    @"Unspecified (non-VCL)",
};


- (CVPixelBufferRef) pixelBufferFromCGImage: (CGImageRef) image
{
    
    CGSize frameSize = CGSizeMake(CGImageGetWidth(image), CGImageGetHeight(image));
    NSDictionary *options = [NSDictionary dictionaryWithObjectsAndKeys:
                             [NSNumber numberWithBool:NO], kCVPixelBufferCGImageCompatibilityKey,
                             [NSNumber numberWithBool:NO], kCVPixelBufferCGBitmapContextCompatibilityKey,
                             nil];
    CVPixelBufferRef pxbuffer = NULL;
//    @{(id)kCVPixelBufferOpenGLESCompatibilityKey: @(NO), (id)kCVPixelBufferPixelFormatTypeKey:@(kCVPixelFormatType_32BGRA)};
    CVReturn status = CVPixelBufferCreate(kCFAllocatorDefault, frameSize.width,
                                          frameSize.height,  kCVPixelFormatType_32BGRA, (CFDictionaryRef) options,
                                          &pxbuffer);
    NSParameterAssert(status == kCVReturnSuccess && pxbuffer != NULL);
    
    CVPixelBufferLockBaseAddress(pxbuffer, 0);
    void *pxdata = CVPixelBufferGetBaseAddress(pxbuffer);
    
    
    CGColorSpaceRef rgbColorSpace = CGColorSpaceCreateDeviceRGB();
    CGContextRef context = CGBitmapContextCreate(pxdata, frameSize.width,
                                                 frameSize.height, 8, CVPixelBufferGetBytesPerRow(pxbuffer), rgbColorSpace,
                                                 kCGImageAlphaNoneSkipLast);
    
    CGContextDrawImage(context, CGRectMake(0, 0, CGImageGetWidth(image),
                                           CGImageGetHeight(image)), image);
    CGColorSpaceRelease(rgbColorSpace);
    CGContextRelease(context);
    
    CVPixelBufferUnlockBaseAddress(pxbuffer, 0);
    
    return pxbuffer;
}


@end
